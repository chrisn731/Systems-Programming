Asst 2 Notes:

1. Takes in a sigle argument a base directory to start scanning
	-> Need to check if exists / can access

2. Two functions that write results to same data structure (screams threads and sempahores)
	a. File Handling (actually does the job of mutating data)
		-> Will update a shared data struct representing frequency of all tokens in
			all files as it scans through the file it has been given.
		-> Check to see if file exists/can access.
		-> If it can access it
			-> If can Lock synchronization mechanism, add entry in shared data structure
				holding filename and total number of tokens scanned. Then unlock
			-> else print an error message and return.
		-> Start tokenzizing the file
			-> Quote from pdf: "All files will consist of ASCII text. 
				All tokens will be space-separated or newline-separated. Tokens will							consist of alphabetic characters and hyphens. Ignore all punctuation 
				and convert all tokens to lowercase in order to ease classification."
			-> Divide each token amount by total number of tokens to get discrete prob of each.
			-> Sort tokens alphabetically (insertion sort).

	b. Directory Handling (Main job is to parse directories)
		-> See if dir is accessible, if so open with opendir and read through contents
			-> If not output error and return gracefully
		-> If a valid directory is found
			-> Create new pthread with copy of directory handling func.
			-> Pass name of path to dir to new thread, a ptr to shared data structure
				and a pointer to synchronization mechanism (aka mutex).
		-> If a valid file is found
			-> Create a new pthread with running file handler.
			-> Pass an argument to that thread that holds the name of the file to be tokenized 						concatenated to the path so far, a ptr to shared data structure, and ptr
				to synchronization method.
		-> Anything that is not a dir or REGULAR file it should ignore and pass over with 
			no warning/error
		-> Join all threads

3. After 2, begin analysis
	-> If there is only one entry in the data structure should emit warning and stop.
	-> Use Jensen-Shannon Distance between each of the distributions.
	-> Compute the mean distribution of the two token distributions you are analyzing.
		-> Construct a new mean token list by iterating through the two and computing the mean of the 			probabilities of any tokens that are the same and halving the probabilities of any
			 that are unique to one distribution.
		-> MeanProb(tokenX) = (FirstDistributionProb(tokenX) + SecondDistributionProb(tokenX)) / 2;
	-> Compute Kullbeck-Leibler Divergence of each distribution.
		-> Youre gonna need to look at this on your own its kinda weird.
	-> Use Jensen-Shannon Distance to compute mean of two Kullbeck Divergences and get a number in [0, .5)

	-> TLDR: Do some fancy math on the words in the file to get a number, if that number is close to another
		number of another file it is likely they have the same words. ggez

4. Print colors for math :)
	-> Print each file pair compared, use colors to denote their range val computed from 3
	-> Red for [0, 0.1]
	-> Yellow  (0.1, 0.15]
	-> Green   (0.15, .2]
	-> Cyan    (.2, .25]
	-> Blue    (.25, .3]
	-> White for all other file pairs
	-> NOTE: The value is colored, not the file names. Ex. "0.01 hi.txt and yurp.txt" only 0.01
		will be colored red.
5. Fin